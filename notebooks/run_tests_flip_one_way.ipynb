{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df4533f-1745-4e54-8f0b-8fd33a9260bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=15, stopwords=True, lime_num_samples=300, shots=None\n",
      "Results => {'num_samples': 500, 'max_features': 15, 'stopwords': True, 'lime_num_samples': 300, 'shots': None, 'accuracy': 0.59, 'lime_time_avg': 0.2419, 'qlime_time_avg': 0.1476, 'overlap_avg': 4.0}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=15, stopwords=True, lime_num_samples=300, shots=100\n",
      "Results => {'num_samples': 500, 'max_features': 15, 'stopwords': True, 'lime_num_samples': 300, 'shots': 100, 'accuracy': 0.6, 'lime_time_avg': 0.228, 'qlime_time_avg': 0.097, 'overlap_avg': 3.4}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=15, stopwords=False, lime_num_samples=300, shots=None\n",
      "Results => {'num_samples': 500, 'max_features': 15, 'stopwords': False, 'lime_num_samples': 300, 'shots': None, 'accuracy': 0.56, 'lime_time_avg': 0.2803, 'qlime_time_avg': 0.2105, 'overlap_avg': 2.6}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=15, stopwords=False, lime_num_samples=300, shots=100\n",
      "Results => {'num_samples': 500, 'max_features': 15, 'stopwords': False, 'lime_num_samples': 300, 'shots': 100, 'accuracy': 0.54, 'lime_time_avg': 0.3977, 'qlime_time_avg': 0.2766, 'overlap_avg': 1.0}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=10, stopwords=True, lime_num_samples=300, shots=None\n",
      "Results => {'num_samples': 500, 'max_features': 10, 'stopwords': True, 'lime_num_samples': 300, 'shots': None, 'accuracy': 0.65, 'lime_time_avg': 0.1371, 'qlime_time_avg': 0.003, 'overlap_avg': 2.8}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=10, stopwords=True, lime_num_samples=300, shots=100\n",
      "Results => {'num_samples': 500, 'max_features': 10, 'stopwords': True, 'lime_num_samples': 300, 'shots': 100, 'accuracy': 0.44, 'lime_time_avg': 0.1517, 'qlime_time_avg': 0.0035, 'overlap_avg': 2.8}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=10, stopwords=False, lime_num_samples=300, shots=None\n",
      "Results => {'num_samples': 500, 'max_features': 10, 'stopwords': False, 'lime_num_samples': 300, 'shots': None, 'accuracy': 0.5, 'lime_time_avg': 0.3077, 'qlime_time_avg': 0.0117, 'overlap_avg': 3.0}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=10, stopwords=False, lime_num_samples=300, shots=100\n",
      "Results => {'num_samples': 500, 'max_features': 10, 'stopwords': False, 'lime_num_samples': 300, 'shots': 100, 'accuracy': 0.56, 'lime_time_avg': 0.2221, 'qlime_time_avg': 0.0133, 'overlap_avg': 3.2}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=5, stopwords=True, lime_num_samples=300, shots=None\n",
      "Results => {'num_samples': 500, 'max_features': 5, 'stopwords': True, 'lime_num_samples': 300, 'shots': None, 'accuracy': 0.52, 'lime_time_avg': 0.2249, 'qlime_time_avg': 0.0023, 'overlap_avg': 3.4}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=5, stopwords=True, lime_num_samples=300, shots=100\n",
      "Results => {'num_samples': 500, 'max_features': 5, 'stopwords': True, 'lime_num_samples': 300, 'shots': 100, 'accuracy': 0.58, 'lime_time_avg': 0.4412, 'qlime_time_avg': 0.0027, 'overlap_avg': 3.4}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=5, stopwords=False, lime_num_samples=300, shots=None\n",
      "Results => {'num_samples': 500, 'max_features': 5, 'stopwords': False, 'lime_num_samples': 300, 'shots': None, 'accuracy': 0.48, 'lime_time_avg': 0.1753, 'qlime_time_avg': 0.0027, 'overlap_avg': 4.6}\n",
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=5, stopwords=False, lime_num_samples=300, shots=100\n",
      "Results => {'num_samples': 500, 'max_features': 5, 'stopwords': False, 'lime_num_samples': 300, 'shots': 100, 'accuracy': 0.48, 'lime_time_avg': 0.2581, 'qlime_time_avg': 0.0037, 'overlap_avg': 5.0}\n",
      "\n",
      "All done! Saved results to 'results_expanded_flips.csv'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classical LIME\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "##############################################################################\n",
    "# PART 0: DATA LOADING AND PREPROCESSING\n",
    "##############################################################################\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes HTML tags and converts to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove anything between <...> tags, then lowercase the text\n",
    "    cleaned = re.sub(r'<.*?>', '', text).lower()\n",
    "    return cleaned\n",
    "\n",
    "def load_imdb_subset(\n",
    "    num_samples=500, \n",
    "    min_df=1, \n",
    "    max_features=10, \n",
    "    stopwords_option=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a subset of IMDb data, returns:\n",
    "      - X_train, X_test (lists of text)\n",
    "      - y_train, y_test (0/1 sentiment)\n",
    "      - vectorizer (CountVectorizer)\n",
    "    \n",
    "    Now with text cleaning for HTML, lowercase, etc.\n",
    "    \"\"\"\n",
    "    data = load_files(\n",
    "        \"./aclImdb/train\", \n",
    "        categories=['pos','neg'], \n",
    "        encoding=\"utf-8\", \n",
    "        decode_error=\"replace\"\n",
    "    )\n",
    "    X_text_all, y_all = data.data, data.target\n",
    "\n",
    "    # Clean text (HTML removal + lowercase)\n",
    "    X_text_all = [clean_text(txt) for txt in X_text_all]\n",
    "\n",
    "    # Shuffle & truncate to num_samples\n",
    "    full_idx = np.arange(len(X_text_all))\n",
    "    np.random.shuffle(full_idx)\n",
    "    subset_idx = full_idx[:num_samples]\n",
    "    X_text = [X_text_all[i] for i in subset_idx]\n",
    "    y = y_all[subset_idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Vectorizer: presence/absence\n",
    "    if stopwords_option:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words='english', \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=None, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "\n",
    "def train_logistic_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    return clf\n",
    "\n",
    "##############################################################################\n",
    "# CLASSICAL LIME\n",
    "##############################################################################\n",
    "\n",
    "def run_classical_lime(\n",
    "    text_sample, clf, vectorizer, \n",
    "    k_features=5, num_samples=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs classical LIME on a single text instance.\n",
    "    Returns the top (word, weight) pairs.\n",
    "    \"\"\"\n",
    "    class_names = [\"negative\", \"positive\"]\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        bow = vectorizer.transform(texts) \n",
    "        return clf.predict_proba(bow)\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_sample,\n",
    "        predict_proba,\n",
    "        num_features=k_features,\n",
    "        num_samples=num_samples  # e.g. 300 or 500\n",
    "    )\n",
    "    return explanation.as_list()  # list of (word, weight)\n",
    "\n",
    "##############################################################################\n",
    "# Q-LIME Pi (Flip Only 1->0)\n",
    "##############################################################################\n",
    "\n",
    "def classical_classifier(features, weights, bias=0.0):\n",
    "    \"\"\"\n",
    "    Simple logistic: score = bias + dot(features, weights).\n",
    "    Then return sigmoid.\n",
    "    \"\"\"\n",
    "    score = bias + np.dot(features, weights)\n",
    "    return 1 / (1 + np.exp(-score))\n",
    "\n",
    "def encode_and_flip(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Encode features -> quantum circuit.\n",
    "    FLIP ONLY if bit == 1 at flip_index (1->0).\n",
    "    \"\"\"\n",
    "    num_qubits = len(features)\n",
    "    dev = qml.device(\"default.qubit\", wires=num_qubits, shots=shots)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit():\n",
    "        for i, f in enumerate(features):\n",
    "            if i == flip_index and f == 1:\n",
    "                # 1->0 => RY(0)\n",
    "                theta = 0.0\n",
    "            else:\n",
    "                # otherwise normal encoding: f * (pi/2)\n",
    "                theta = f * (np.pi / 2)\n",
    "            qml.RY(theta, wires=i)\n",
    "        return qml.probs(wires=range(num_qubits))\n",
    "\n",
    "    return circuit()\n",
    "\n",
    "def sample_state(probabilities):\n",
    "    \"\"\"\n",
    "    Sample an integer state index from the distribution.\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    cumsum = 0.0\n",
    "    for idx, p in enumerate(probabilities):\n",
    "        cumsum += p\n",
    "        if r <= cumsum:\n",
    "            return idx\n",
    "    return len(probabilities) - 1\n",
    "\n",
    "def measure_and_map_to_classical(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Run the circuit, measure, return a binary array for the top-likelihood state.\n",
    "    \"\"\"\n",
    "    probs = encode_and_flip(features, flip_index=flip_index, shots=shots)\n",
    "    measured_state = sample_state(probs)\n",
    "    num_qubits = len(features)\n",
    "    bin_string = f\"{measured_state:0{num_qubits}b}\"\n",
    "    return [int(bit) for bit in bin_string]\n",
    "\n",
    "def quantum_lime_explanation(\n",
    "    features, weights, bias=0.0, shots=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Flip only features that are 1 -> 0.\n",
    "    Return array of shape (n_features,) with:\n",
    "       Delta f_k = (original_pred - new_pred).\n",
    "    \"\"\"\n",
    "    original_pred = classical_classifier(features, weights, bias=bias)\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        new_vec = measure_and_map_to_classical(features, flip_index=i, shots=shots)\n",
    "        new_pred = classical_classifier(new_vec, weights, bias=bias)\n",
    "        return original_pred - new_pred\n",
    "\n",
    "    # Flip only bits that are 1\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) if val == 1\n",
    "        }\n",
    "        for future in futures:\n",
    "            i = futures[future]\n",
    "            contributions[i] = future.result()\n",
    "\n",
    "    return contributions\n",
    "\n",
    "##############################################################################\n",
    "# EXPERIMENTAL ROUTINE\n",
    "##############################################################################\n",
    "\n",
    "def run_experiment(\n",
    "    num_samples=500,\n",
    "    min_df=1,\n",
    "    max_features=10,\n",
    "    stopwords_option=True,\n",
    "    lime_num_samples=300,\n",
    "    shots=None,\n",
    "    n_test_explanations=5\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load data with given params (includes text cleaning)\n",
    "    2) Train logistic classifier\n",
    "    3) Evaluate test accuracy\n",
    "    4) Pick n_test_explanations random samples\n",
    "    5) For each, run classical LIME vs. Q-LIME Pi\n",
    "    6) Return summary stats\n",
    "    \"\"\"\n",
    "    # A) Load data\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=num_samples,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        stopwords_option=stopwords_option\n",
    "    )\n",
    "    # B) Train model\n",
    "    clf = train_logistic_classifier(X_train, y_train, vectorizer)\n",
    "\n",
    "    # Evaluate\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "\n",
    "    logistic_weights = clf.coef_[0]\n",
    "    bias = clf.intercept_[0]\n",
    "\n",
    "    # We'll track times & top-feature overlap\n",
    "    lime_times = []\n",
    "    qlime_times = []\n",
    "    overlaps = []\n",
    "\n",
    "    # Random samples for explanation\n",
    "    n_test = len(X_test)\n",
    "    sample_indices = np.random.choice(n_test, size=n_test_explanations, replace=False)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        text_sample = X_test[idx]\n",
    "\n",
    "        # 1) Classical LIME\n",
    "        start_lime = time.time()\n",
    "        explanation_lime = run_classical_lime(\n",
    "            text_sample, clf, vectorizer, \n",
    "            k_features=5, num_samples=lime_num_samples\n",
    "        )\n",
    "        lime_time = time.time() - start_lime\n",
    "        lime_times.append(lime_time)\n",
    "\n",
    "        # parse top features\n",
    "        lime_dict = dict(explanation_lime)\n",
    "        top_words_lime = sorted(\n",
    "            lime_dict.keys(),\n",
    "            key=lambda w: abs(lime_dict[w]),\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        # 2) Q-LIME Pi\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "\n",
    "        start_qlime = time.time()\n",
    "        contributions_qlime = quantum_lime_explanation(\n",
    "            bin_features, logistic_weights, bias=bias, shots=shots\n",
    "        )\n",
    "        qlime_time = time.time() - start_qlime\n",
    "        qlime_times.append(qlime_time)\n",
    "\n",
    "        # top 5 (by absolute value)\n",
    "        nonzero_indices = [\n",
    "            (i, abs(contributions_qlime[i])) \n",
    "            for i in range(len(contributions_qlime))\n",
    "        ]\n",
    "        top_indices_qlime = sorted(nonzero_indices, key=lambda x: x[1], reverse=True)[:5]\n",
    "        top_words_qlime = [\n",
    "            vectorizer.get_feature_names_out()[i2]\n",
    "            for (i2, val) in top_indices_qlime\n",
    "        ]\n",
    "\n",
    "        # measure overlap\n",
    "        overlap = set(top_words_lime).intersection(set(top_words_qlime))\n",
    "        overlaps.append(len(overlap))\n",
    "\n",
    "    # Summary\n",
    "    results = {\n",
    "        \"accuracy\": round(test_acc, 4),\n",
    "        \"lime_time_avg\": round(np.mean(lime_times), 4),\n",
    "        \"qlime_time_avg\": round(np.mean(qlime_times), 4),\n",
    "        \"overlap_avg\": round(np.mean(overlaps), 4),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "##############################################################################\n",
    "# MAIN\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    # Parameter grid to systematically vary certain settings\n",
    "    param_grid = {\n",
    "        \"num_samples\": [500],\n",
    "        \"max_features\": [15, 10, 5],\n",
    "        \"stopwords_option\": [True, False],\n",
    "        \"lime_num_samples\": [300],\n",
    "        # Shots: None => analytic mode, 100 => finite sampling\n",
    "        \"shots\": [None, 100],\n",
    "    }\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    all_results = []\n",
    "\n",
    "    for combo in combos:\n",
    "        (num_samples_, max_features_, stopwords_, lime_samps_, shots_) = combo\n",
    "        \n",
    "        print(\"\\n==================================\")\n",
    "        print(f\"Running experiment with: \"\n",
    "              f\"num_samples={num_samples_}, \"\n",
    "              f\"max_features={max_features_}, \"\n",
    "              f\"stopwords={stopwords_}, \"\n",
    "              f\"lime_num_samples={lime_samps_}, \"\n",
    "              f\"shots={shots_}\")\n",
    "        \n",
    "        res = run_experiment(\n",
    "            num_samples=num_samples_,\n",
    "            max_features=max_features_,\n",
    "            stopwords_option=stopwords_,\n",
    "            lime_num_samples=lime_samps_,\n",
    "            shots=shots_,\n",
    "            n_test_explanations=5\n",
    "        )\n",
    "        res_row = {\n",
    "            \"num_samples\": num_samples_,\n",
    "            \"max_features\": max_features_,\n",
    "            \"stopwords\": stopwords_,\n",
    "            \"lime_num_samples\": lime_samps_,\n",
    "            \"shots\": shots_,\n",
    "            \"accuracy\": res[\"accuracy\"],\n",
    "            \"lime_time_avg\": res[\"lime_time_avg\"],\n",
    "            \"qlime_time_avg\": res[\"qlime_time_avg\"],\n",
    "            \"overlap_avg\": res[\"overlap_avg\"],\n",
    "        }\n",
    "        print(\"Results =>\", res_row)\n",
    "        all_results.append(res_row)\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"results_expanded_flips.csv\", index=False)\n",
    "    print(\"\\nAll done! Saved results to 'results_expanded_flips.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64942c-9d2d-4dba-b9cb-825468a3aeff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
