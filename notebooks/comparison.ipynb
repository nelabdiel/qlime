{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b0b4cf4-ff76-4b2c-a587-af69fb240382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN EXAMPLE DEMO:\n",
      "Test Accuracy = 0.64\n",
      "\n",
      "=====================================\n",
      "TEXT SAMPLE (index=42): the western society has been fed ideas about india being a poor country. movies like these only make those beliefs stronger. such illustrations make it all the more difficult for indians to be accepted abroad. agreed there are poor and homeless in india, but why is there no representation of educated people if not the successful ones.i totally hated the idea of the movie portraying patrick swayze as another mother teressa. in my opinion this movie has shown india in a very bad light giving wrong notions. it is unjust to discuss only one aspect of the society. exactly the reason why people ask me, \"when we go to india, can we hire an elephant right outside the airport so we do not have to walk on the roads so full of filth and snakes?\"those who want a second opinion on contemporary indian society should watch \"monsoon wedding\".\n",
      "True Label = 0 | Predicted = 0\n",
      "\n",
      "[Classical LIME] time = 0.756 sec\n",
      "   movie: -0.049\n",
      "   like: 0.021\n",
      "   filth: -0.000\n",
      "   poor: -0.000\n",
      "   go: -0.000\n",
      "\n",
      "[Q-LIME Pi] time = 0.010 sec\n",
      "   Feature like (idx=5): -0.028\n",
      "   Feature movie (idx=6): -0.049\n",
      "\n",
      "Top-5 words (Classical LIME): ['movie', 'like', 'filth', 'poor', 'go']\n",
      "Top-5 words (Q-LIME Pi):       ['movie', 'like']\n",
      "Overlap in top features: {'like', 'movie'}\n",
      "-------------------------------------\n",
      "\n",
      "=====================================\n",
      "TEXT SAMPLE (index=65): a decent sequel, but does not pack the punch of the original. a murderous screenwriter(judd nelson)assumes new identities in order to direct his own novel cabin by the lake. still ruthless killing, but movie seems very tongue-in-cheek. any humor is not of the funny kind. total project seems to have the quality of a quickie and at times nelson is way over the top. this movie is about a script being rewritten before going to the screen...this should have happened to this script.\n",
      "True Label = 0 | Predicted = 0\n",
      "\n",
      "[Classical LIME] time = 0.389 sec\n",
      "   movie: -0.049\n",
      "   but: -0.000\n",
      "   a: -0.000\n",
      "   decent: -0.000\n",
      "   sequel: -0.000\n",
      "\n",
      "[Q-LIME Pi] time = 0.004 sec\n",
      "   Feature movie (idx=6): -0.049\n",
      "\n",
      "Top-5 words (Classical LIME): ['movie', 'but', 'a', 'decent', 'sequel']\n",
      "Top-5 words (Q-LIME Pi):       ['movie']\n",
      "Overlap in top features: {'movie'}\n",
      "-------------------------------------\n",
      "\n",
      "=====================================\n",
      "TEXT SAMPLE (index=28): thhe2 is entertaining in that you'll laugh a lot and cringe and probably say \"oh sh*t!\" and \"get your face away from the goddamn hole you dumb**s\" or things along those lines but i don't know if its really worth seeing- i was very annoyed throughout the entirety with the horrible military characters who don't seem to know the first thing about combat.yes there was more violence, gore, and a higher body count than the first one but i am still am debating whether that cancels out my feeling throughout the whole movie about how ridiculous it is (and not a good ridiculousness like dead alive or feast). my time would have been better spent watching aja's remake for the 5th time.so go for some laughs, or go for some gore, but don't go hoping to come out of it satisfied.\n",
      "True Label = 0 | Predicted = 0\n",
      "\n",
      "[Classical LIME] time = 0.581 sec\n",
      "   don: -0.176\n",
      "   movie: -0.043\n",
      "   really: 0.032\n",
      "   time: 0.028\n",
      "   good: -0.020\n",
      "\n",
      "[Q-LIME Pi] time = 0.030 sec\n",
      "   Feature don (idx=0): -0.254\n",
      "   Feature good (idx=2): 0.033\n",
      "   Feature like (idx=5): -0.184\n",
      "   Feature movie (idx=6): -0.146\n",
      "   Feature really (idx=7): -0.200\n",
      "   Feature time (idx=9): 0.070\n",
      "\n",
      "Top-5 words (Classical LIME): ['don', 'movie', 'really', 'time', 'good']\n",
      "Top-5 words (Q-LIME Pi):       ['don', 'really', 'like', 'movie', 'time']\n",
      "Overlap in top features: {'really', 'movie', 'time', 'don'}\n",
      "-------------------------------------\n",
      "\n",
      "=====================================\n",
      "TEXT SAMPLE (index=1): what network was , diagnosis murder on? i thought it was cbs. am i right or?? also, back in those days, the actual production h.q. was near about the van nuys airport. i surely remember, because i practically made nearly two episodes in those daze. more. i remember the early days. i had found an article in reader's digest giving this actor/writer a clue to a terrific episode. so just for suggesting it i was awarded. awarded or not, i sadly didn't develop it, and was cut out of it all due to poor publicity of mine. so as a justification i learned as i always have, the hard way... roll the dice..craps!!! just a side bar on mr. van dyke. he had a house in the brentwood area on chalon road and it was an incredible party house. dick had a terrific sense of modernism when he built that house.\n",
      "True Label = 1 | Predicted = 0\n",
      "\n",
      "[Classical LIME] time = 0.920 sec\n",
      "   just: -0.028\n",
      "   what: -0.000\n",
      "   network: -0.000\n",
      "   was: -0.000\n",
      "   diagnosis: -0.000\n",
      "\n",
      "[Q-LIME Pi] time = 0.010 sec\n",
      "   Feature just (idx=4): -0.028\n",
      "\n",
      "Top-5 words (Classical LIME): ['just', 'what', 'network', 'was', 'diagnosis']\n",
      "Top-5 words (Q-LIME Pi):       ['just']\n",
      "Overlap in top features: {'just'}\n",
      "-------------------------------------\n",
      "\n",
      "=====================================\n",
      "TEXT SAMPLE (index=24): i originally scored sarah's show with a nice fat 8, but i've struggled a bit with her humor of late and a thin 7 is what's settled in. i shall explain.you will either like sarah's humor, or you won't. if you don't, i doubt anyone could persuade you. you folks know who you are and it's perfectly fine, but then you know that too. moving on, the first season gave us fantastic bits about sarah, her friends and family, and her pursuits in life. in one memorable episode, she is \"pulled over\" by officer jay whom she meets for the first time. - \"do you know why i am standing here?\" he asks. \"because you got all c's in high school?\" she quizzically replies. it seemed to be a genuine question. - that is funny stuff in my book. sarah can come at you from odd angles. in another episode, her affair with god was notably funny. god being petty and jealous added wonderfully to the joke. it is clever, it is a twisted view, but she would show us the truth in the humor and we laughed.then, came the second season. while still not without some new and inventive comedy, we seem to have slipped somewhat into banal poop and fart jokes, quite simply. i get some good laughs here and there, but much of it seems like filler while she, and the writers, struggle to foment some original material. sophomoric and tiresome are the feelings i have for the episodes lately, but i have been gutting it out for the gems i do find (the turtle) and waiting for her to turn it around. i was a fan of her \"jesus is magic\" routine and would like to think that i understand what she is capable of. let's get back to that.\n",
      "True Label = 1 | Predicted = 0\n",
      "\n",
      "[Classical LIME] time = 1.210 sec\n",
      "   don: -0.180\n",
      "   time: 0.027\n",
      "   good: -0.023\n",
      "   like: 0.019\n",
      "   struggle: -0.000\n",
      "\n",
      "[Q-LIME Pi] time = 0.014 sec\n",
      "   Feature don (idx=0): -0.153\n",
      "   Feature good (idx=2): -0.178\n",
      "   Feature like (idx=5): -0.004\n",
      "   Feature time (idx=9): 0.026\n",
      "\n",
      "Top-5 words (Classical LIME): ['don', 'time', 'good', 'like', 'struggle']\n",
      "Top-5 words (Q-LIME Pi):       ['good', 'don', 'time', 'like']\n",
      "Overlap in top features: {'like', 'time', 'don', 'good'}\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "RUN BENCHMARK TEST:\n",
      "\n",
      "=== BENCHMARK RESULTS (Flip Only 1->0) ===\n",
      "max_feats | Acc  | LIME_time | QLIME_time | Overlap\n",
      "        5 | 0.480 | 0.925   | 0.005      | 2.50\n",
      "       10 | 0.510 | 0.935   | 0.015      | 3.60\n",
      "       15 | 0.650 | 0.670   | 0.052      | 3.10\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classical LIME\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "##############################################################################\n",
    "# PART 0: DATA LOADING AND PREPROCESSING\n",
    "##############################################################################\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags and convert to lowercase.\"\"\"\n",
    "    return re.sub(r'<.*?>', '', text).lower()\n",
    "    \n",
    "def load_imdb_subset(num_samples=1000, min_df=2, max_features=20):\n",
    "    \"\"\"\n",
    "    Loads a subset of IMDb data, returns:\n",
    "      - X_train, X_test (lists of text)\n",
    "      - y_train, y_test (0/1 sentiment)\n",
    "      - vectorizer (CountVectorizer)\n",
    "    \"\"\"\n",
    "    data = load_files(\"./aclImdb/train\", categories=['pos','neg'],\n",
    "                      encoding=\"utf-8\", decode_error=\"replace\")\n",
    "    X_text, y = data.data, data.target\n",
    "\n",
    "    # Clean text: remove HTML, lowercase\n",
    "    X_text = [clean_text(txt) for txt in X_text]\n",
    "\n",
    "    # Shuffle & truncate\n",
    "    idx = np.arange(num_samples)\n",
    "    np.random.shuffle(idx)\n",
    "    X_text = [X_text[i] for i in idx]\n",
    "    y = y[idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Binary presence/absence vectorizer\n",
    "    vectorizer = CountVectorizer(binary=True, stop_words='english',\n",
    "                                 min_df=min_df, max_features=max_features)\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "##############################################################################\n",
    "# PART 1: TRAIN A SIMPLE SENTIMENT CLASSIFIER\n",
    "##############################################################################\n",
    "\n",
    "def train_logistic_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"Trains logistic regression on the binary presence/absence of words.\"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    return clf\n",
    "\n",
    "##############################################################################\n",
    "# PART 2: CLASSICAL LIME EXPLANATIONS\n",
    "##############################################################################\n",
    "\n",
    "def run_classical_lime(text_sample, clf, vectorizer, k_features=5):\n",
    "    \"\"\"\n",
    "    Runs classical LIME on a single text instance.\n",
    "    Returns list of (word, weight) pairs.\n",
    "    \"\"\"\n",
    "    class_names = [\"negative\", \"positive\"]\n",
    "    explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        bow = vectorizer.transform(texts)\n",
    "        return clf.predict_proba(bow)\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_sample,\n",
    "        predict_proba,\n",
    "        num_features=k_features\n",
    "    )\n",
    "    return explanation.as_list()\n",
    "\n",
    "##############################################################################\n",
    "# PART 3: Q-LIME Pi (Quantum LIME) Implementation (flip only 1->0)\n",
    "##############################################################################\n",
    "\n",
    "def classical_classifier(features, weights):\n",
    "    \"\"\"Simple logistic: dot(features, weights), then sigmoid.\"\"\"\n",
    "    score = np.dot(features, weights)\n",
    "    return 1 / (1 + np.exp(-score))\n",
    "\n",
    "def encode_and_flip(features, flip_index=None):\n",
    "    \"\"\"\n",
    "    Encode features into a quantum circuit.\n",
    "    Flip only if bit == 1 at flip_index (1->0).\n",
    "    \"\"\"\n",
    "    dev = qml.device(\"default.qubit\", wires=len(features), shots=None)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit():\n",
    "        for i, f in enumerate(features):\n",
    "            if i == flip_index and f == 1:\n",
    "                theta = 0.0  # flip 1->0\n",
    "            else:\n",
    "                theta = f * (np.pi / 2)\n",
    "            qml.RY(theta, wires=i)\n",
    "        return qml.probs(wires=range(len(features)))\n",
    "\n",
    "    return circuit()\n",
    "\n",
    "def sample_state(probabilities):\n",
    "    \"\"\"Randomly sample an integer state index from a prob distribution.\"\"\"\n",
    "    r = random.random()\n",
    "    cumsum = 0.0\n",
    "    for idx, p in enumerate(probabilities):\n",
    "        cumsum += p\n",
    "        if r <= cumsum:\n",
    "            return idx\n",
    "    return len(probabilities) - 1\n",
    "\n",
    "def measure_and_map_to_classical(features, flip_index=None):\n",
    "    \"\"\"Measure the quantum state, get a single classical bitstring.\"\"\"\n",
    "    probs = encode_and_flip(features, flip_index)\n",
    "    measured_state = sample_state(probs)\n",
    "    num_qubits = len(features)\n",
    "    bin_string = f\"{measured_state:0{num_qubits}b}\"\n",
    "    return [int(bit) for bit in bin_string]\n",
    "\n",
    "def quantum_lime_explanation(features, weights):\n",
    "    \"\"\"\n",
    "    For each bit that is 1, flip it to 0, measure difference in classifier output.\n",
    "    Return array of shape (n_features, ) with contributions.\n",
    "    \"\"\"\n",
    "    original_pred = classical_classifier(features, weights)\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        new_vec = measure_and_map_to_classical(features, flip_index=i)\n",
    "        new_pred = classical_classifier(new_vec, weights)\n",
    "        return original_pred - new_pred\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) if val == 1\n",
    "        }\n",
    "        for fut in futures:\n",
    "            i = futures[fut]\n",
    "            contributions[i] = fut.result()\n",
    "\n",
    "    return contributions\n",
    "\n",
    "##############################################################################\n",
    "# PART 4A: EXAMPLE RUN (like your current approach)\n",
    "##############################################################################\n",
    "\n",
    "def run_example():\n",
    "    # 1) Load IMDb subset (small sample)\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=500,\n",
    "        min_df=1,\n",
    "        max_features=10\n",
    "    )\n",
    "\n",
    "    # 2) Train logistic regression\n",
    "    clf = train_logistic_classifier(X_train, y_train, vectorizer)\n",
    "\n",
    "    # Evaluate\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "    print(f\"Test Accuracy = {test_acc:.2f}\")\n",
    "\n",
    "    # Extract logistic weights\n",
    "    logistic_weights = clf.coef_[0]\n",
    "    bias = clf.intercept_[0]  # unused in classical_classifier\n",
    "\n",
    "    # 3) Pick test samples for explanation\n",
    "    num_samples_to_explain = 5\n",
    "    sample_indices = np.random.choice(len(X_test), size=num_samples_to_explain, replace=False)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        text_sample = X_test[idx]\n",
    "        true_label = y_test[idx]\n",
    "\n",
    "        print(\"\\n=====================================\")\n",
    "        print(f\"TEXT SAMPLE (index={idx}): {text_sample}\")\n",
    "        print(f\"True Label = {true_label} | Predicted = {clf.predict(vectorizer.transform([text_sample]))[0]}\")\n",
    "\n",
    "        #--- Classical LIME\n",
    "        start_lime = time.time()\n",
    "        explanation_lime = run_classical_lime(text_sample, clf, vectorizer, k_features=5)\n",
    "        lime_time = time.time() - start_lime\n",
    "\n",
    "        print(f\"\\n[Classical LIME] time = {lime_time:.3f} sec\")\n",
    "        for word, weight in explanation_lime:\n",
    "            print(f\"   {word}: {weight:.3f}\")\n",
    "\n",
    "        #--- Q-LIME Pi\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "\n",
    "        start_qlime = time.time()\n",
    "        contributions_qlime = quantum_lime_explanation(bin_features, logistic_weights)\n",
    "        qlime_time = time.time() - start_qlime\n",
    "\n",
    "        print(f\"\\n[Q-LIME Pi] time = {qlime_time:.3f} sec\")\n",
    "        for i, contrib in enumerate(contributions_qlime):\n",
    "            if abs(contrib) > 1e-7:\n",
    "                feat_name = vectorizer.get_feature_names_out()[i]\n",
    "                print(f\"   Feature {feat_name} (idx={i}): {contrib:.3f}\")\n",
    "\n",
    "        #--- Compare top-5\n",
    "        lime_dict = dict(explanation_lime)\n",
    "        top_words_lime = sorted(lime_dict.keys(), key=lambda w: abs(lime_dict[w]), reverse=True)[:5]\n",
    "\n",
    "        nonzero_indices = [(i, abs(contributions_qlime[i]))\n",
    "                           for i in range(len(contributions_qlime))\n",
    "                           if abs(contributions_qlime[i]) > 1e-7]\n",
    "        top_indices_qlime = sorted(nonzero_indices, key=lambda x: x[1], reverse=True)[:5]\n",
    "        top_words_qlime = [vectorizer.get_feature_names_out()[idx2] for (idx2, val) in top_indices_qlime]\n",
    "\n",
    "        overlap = set(top_words_lime).intersection(set(top_words_qlime))\n",
    "        print(f\"\\nTop-5 words (Classical LIME): {top_words_lime}\")\n",
    "        print(f\"Top-5 words (Q-LIME Pi):       {top_words_qlime}\")\n",
    "        print(f\"Overlap in top features: {overlap}\")\n",
    "        print(\"-------------------------------------\")\n",
    "\n",
    "##############################################################################\n",
    "# PART 4B: BENCHMARK TEST (for multiple max_features, etc.)\n",
    "##############################################################################\n",
    "\n",
    "def benchmark_test():\n",
    "    \"\"\"\n",
    "    Systematically vary max_features, etc., measure:\n",
    "      - test accuracy\n",
    "      - LIME explanation time\n",
    "      - QLIME Pi explanation time\n",
    "      - overlap in top-5 features\n",
    "    We'll just do a small loop for demonstration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameter combos to test\n",
    "    max_features_list = [5, 10, 15]\n",
    "    results = []\n",
    "\n",
    "    for mf in max_features_list:\n",
    "        # Load data\n",
    "        X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "            num_samples=500,\n",
    "            min_df=1,\n",
    "            max_features=mf\n",
    "        )\n",
    "        clf = train_logistic_classifier(X_train, y_train, vectorizer)\n",
    "\n",
    "        X_test_bow = vectorizer.transform(X_test)\n",
    "        acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "\n",
    "        logistic_weights = clf.coef_[0]\n",
    "\n",
    "        # We'll pick 10 random samples to measure average LIME & QLIME times, overlap\n",
    "        sample_indices = np.random.choice(len(X_test), size=10, replace=False)\n",
    "\n",
    "        lime_times = []\n",
    "        qlime_times = []\n",
    "        overlaps = []\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            text_sample = X_test[idx]\n",
    "\n",
    "            #--- LIME\n",
    "            start_lime = time.time()\n",
    "            explanation_lime = run_classical_lime(text_sample, clf, vectorizer, k_features=5)\n",
    "            lime_times.append(time.time() - start_lime)\n",
    "\n",
    "            #--- QLIME\n",
    "            bow = vectorizer.transform([text_sample]).toarray()[0]\n",
    "            start_qlime = time.time()\n",
    "            contributions_qlime = quantum_lime_explanation(bow, logistic_weights)\n",
    "            qlime_times.append(time.time() - start_qlime)\n",
    "\n",
    "            # Overlap\n",
    "            lime_dict = dict(explanation_lime)\n",
    "            top_words_lime = sorted(lime_dict.keys(), key=lambda w: abs(lime_dict[w]), reverse=True)[:5]\n",
    "            nonzero_indices = [(i, abs(contributions_qlime[i]))\n",
    "                               for i in range(len(contributions_qlime))\n",
    "                               if abs(contributions_qlime[i]) > 1e-7]\n",
    "            top_indices_qlime = sorted(nonzero_indices, key=lambda x: x[1], reverse=True)[:5]\n",
    "            top_words_qlime = [vectorizer.get_feature_names_out()[idx2] for (idx2, val) in top_indices_qlime]\n",
    "\n",
    "            overlap = set(top_words_lime).intersection(set(top_words_qlime))\n",
    "            overlaps.append(len(overlap))\n",
    "\n",
    "        results.append({\n",
    "            \"max_features\": mf,\n",
    "            \"test_accuracy\": round(acc, 3),\n",
    "            \"lime_time_avg\": round(np.mean(lime_times), 3),\n",
    "            \"qlime_time_avg\": round(np.mean(qlime_times), 3),\n",
    "            \"overlap_avg\": round(np.mean(overlaps), 2)\n",
    "        })\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\n=== BENCHMARK RESULTS (Flip Only 1->0) ===\")\n",
    "    print(\"max_feats | Acc  | LIME_time | QLIME_time | Overlap\")\n",
    "    for r in results:\n",
    "        print(f\"{r['max_features']:9d} | \"\n",
    "              f\"{r['test_accuracy']:.3f} | \"\n",
    "              f\"{r['lime_time_avg']:.3f}   | \"\n",
    "              f\"{r['qlime_time_avg']:.3f}      | \"\n",
    "              f\"{r['overlap_avg']:.2f}\")\n",
    "\n",
    "##############################################################################\n",
    "# MAIN\n",
    "##############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Run the example approach (5 random samples)\n",
    "    print(\"RUN EXAMPLE DEMO:\")\n",
    "    run_example()\n",
    "\n",
    "    # 2) Run a small benchmark test for different max_features\n",
    "    print(\"\\n\\nRUN BENCHMARK TEST:\")\n",
    "    benchmark_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceccceca-f76f-42d0-a92a-8eef6a08ec28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
